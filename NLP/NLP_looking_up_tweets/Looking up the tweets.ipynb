{"cells":[{"cell_type":"markdown","metadata":{"id":"HojYv92k8CxA"},"source":["# Locality Sensitive Hashing (LSH)\n","\n","You will see how locality sensitive hashing works. Let's get started by importing\n","the required functions!\n","\n","If you are running this notebook in your local computer, don't forget to\n","download the twitter samples and stopwords from nltk.\n","\n","```\n","nltk.download('stopwords')\n","nltk.download('twitter_samples')\n","```\n"],"id":"HojYv92k8CxA"},{"cell_type":"markdown","metadata":{"id":"9HovjqJK8CxE"},"source":["## Table of Contents\n","    \n","- [LSH and Document Search](#1)\n","  - [1 - Getting the Document Embeddings](#1-1)     \n","  - [2 - Looking up the Tweets](#1-2)\n","  - [3 - Finding the most Similar Tweets with LSH](#1-3)\n","  - [4 - Getting the Hash Number for a Vector](#1-4)\n","  - [5 - Creating a Hash Table](#1-5)\n","  - [6 - Creating all Hash Tables](#1-6)"],"id":"9HovjqJK8CxE"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzP8hJqA8CxG","executionInfo":{"status":"ok","timestamp":1715317383835,"user_tz":-120,"elapsed":2062,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"501a63df-b946-466e-b85e-426a821c48e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/Notebook_files/NLP_looking_up_tweets\n"]}],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')\n","#%cd /content/drive/MyDrive/Colab Notebooks/Notebook_files/NLP_looking_up_tweets\n","\n","import pdb\n","import pickle\n","import string\n","\n","import time\n","\n","import nltk\n","import numpy as np\n","from nltk.corpus import stopwords, twitter_samples\n","\n","from utils import (cosine_similarity, get_dict,\n","                   process_tweet)\n","from os import getcwd\n","\n","import w4_unittest"],"id":"nzP8hJqA8CxG"},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8qGtpN38CxH"},"outputs":[],"source":["# add folder, tmp2, from our local workspace containing pre-downloaded corpora files to nltk's data path\n","filePath = f\"{getcwd()}/tmp2/\"\n","nltk.data.path.append(filePath)"],"id":"u8qGtpN38CxH"},{"cell_type":"markdown","metadata":{"id":"DC11RnNk8CxH"},"source":["<a name=\"1\"></a>\n","#   LSH and Document Search\n","\n","In this part of the Tutorial, you will implement k-nearest neighbors using locality sensitive hashing.\n","You will then apply this to document search.\n","\n","* Process the tweets and represent each tweet as a vector (represent a\n","document with a vector embedding).\n","* Use locality sensitive hashing and k nearest neighbors to find tweets\n","that are similar to a given tweet."],"id":"DC11RnNk8CxH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WHFxJeO8CxH"},"outputs":[],"source":["# get the positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","all_tweets = all_positive_tweets + all_negative_tweets\n"],"id":"5WHFxJeO8CxH"},{"cell_type":"markdown","source":["<a name=\"1-1\"></a>\n","## 1.1 Getting the Document Embeddings\n","\n","###  The Word Embeddings Data for English Words\n","\n","The full dataset for English embeddings is about 3.64 gigabytes. To prevent the Coursera workspace from\n","crashing, we've extracted a subset of the embeddings for the words that you'll\n","use in this Tutorial.\n"],"metadata":{"id":"iqbAWxNiD4Jr"},"id":"iqbAWxNiD4Jr"},{"cell_type":"code","source":["en_embeddings_subset = pickle.load(open(\"./data/en_embeddings.p\", \"rb\"))"],"metadata":{"id":"QFvb-KYuENiS"},"id":"QFvb-KYuENiS","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQ8FFDW58CxI"},"source":["\n","\n","### Bag-of-words (BOW) Document Models\n","Text documents are sequences of words.\n","* The ordering of words makes a difference. For example, sentences \"Apple pie is\n","better than pepperoni pizza.\" and \"Pepperoni pizza is better than apple pie\"\n","have opposite meanings due to the word ordering.\n","* However, for some applications, ignoring the order of words can allow\n","us to train an efficient and still effective model.\n","* This approach is called Bag-of-words document model.\n","\n","### Document Embeddings\n","* Document embedding is created by summing up the embeddings of all words\n","in the document.\n","* If we don't know the embedding of some word, we can ignore that word."],"id":"GQ8FFDW58CxI"},{"cell_type":"markdown","metadata":{"id":"geJAOUKx8CxI"},"source":["<a name=\"1-1-4\"></a>\n","\n","### Function 'get_document_embedding'\n","* The function `get_document_embedding()` encodes entire document as a \"document\" embedding.\n","* It takes in a document (as a string) and a dictionary, `en_embeddings`\n","* It processes the document, and looks up the corresponding embedding of each word.\n","* It then sums them up and returns the sum of all word vectors of that processed tweet."],"id":"geJAOUKx8CxI"},{"cell_type":"markdown","metadata":{"id":"mkc-0ral8CxI"},"source":["<details>\n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> You can handle missing words easier by using the `get()` method of the python dictionary instead of the bracket notation (i.e. \"[ ]\").</li>\n","    <li> The default value for missing word should be the zero vector. Numpy will <a href=\"https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" > broadcast </a> simple 0 scalar into a vector of zeros during the summation.</li>\n","    <li>Alternatively, skip the addition if a word is not in the dictonary. </li>\n","    <li>  You can use your `process_tweet()` function which allows you to process the tweet. The function just takes in a tweet and returns a list of words.</li>\n","</ul>\n","</p>"],"id":"mkc-0ral8CxI"},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hUI39rP8CxJ"},"outputs":[],"source":["# UNQ_C12 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","def get_document_embedding(tweet, en_embeddings, process_tweet=process_tweet):\n","    '''\n","    Input:\n","        - tweet: a string\n","        - en_embeddings: a dictionary of word embeddings\n","    Output:\n","        - doc_embedding: sum of all word embeddings in the tweet\n","    '''\n","    doc_embedding = np.zeros(300)\n","\n","    # process the document into a list of words (process the tweet)\n","    processed_doc = process_tweet(tweet)\n","    for word in processed_doc:\n","        # add the word embedding to the running total for the document embedding\n","        doc_embedding += en_embeddings.get(word, np.zeros(300))\n","\n","    return doc_embedding"],"id":"9hUI39rP8CxJ"},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"mPr4Vg9w8CxJ","executionInfo":{"status":"ok","timestamp":1715152719373,"user_tz":-120,"elapsed":1019,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"bcccd287-57e1-4b75-9bd1-4db1f4485e8b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])"]},"metadata":{},"execution_count":14}],"source":["# UNQ_C13 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","# testing your function\n","custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n","tweet_embedding = get_document_embedding(custom_tweet, en_embeddings_subset)\n","tweet_embedding[-5:]"],"id":"mPr4Vg9w8CxJ"},{"cell_type":"markdown","metadata":{"id":"ZrkxYjLC8CxK"},"source":["**Expected output**:\n","\n","```\n","array([-0.00268555, -0.15378189, -0.55761719, -0.07216644, -0.32263184])\n","```"],"id":"ZrkxYjLC8CxK"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GERLXSvq8CxK","executionInfo":{"status":"ok","timestamp":1715152725597,"user_tz":-120,"elapsed":303,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"0a274a21-7a20-4bf9-a874-5ac5a8b69a3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m All tests passed\n"]}],"source":["#Â Test your function\n","w4_unittest.test_get_document_embedding(get_document_embedding)"],"id":"GERLXSvq8CxK"},{"cell_type":"code","execution_count":null,"id":"2e2a6690","metadata":{"id":"2e2a6690"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EKzzc5-y8CxL"},"source":["<a name=\"1-1-5\"></a>\n","### Function 'get_document_vecs'\n","\n","#### Store all document vectors into a dictionary\n","Now, let's store all the tweet embeddings into a dictionary.\n","Implement `get_document_vecs()`"],"id":"EKzzc5-y8CxL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VyV5UAI8CxL"},"outputs":[],"source":["# UNQ_C14 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","def get_document_vecs(all_docs, en_embeddings, get_document_embedding=get_document_embedding):\n","    '''\n","    Input:\n","        - all_docs: list of strings - all tweets in our dataset.\n","        - en_embeddings: dictionary with words as the keys and their embeddings as the values.\n","    Output:\n","        - document_vec_matrix: matrix of tweet embeddings.\n","        - ind2Doc_dict: dictionary with indices of tweets in vecs as keys and their embeddings as the values.\n","    '''\n","\n","    # the dictionary's key is an index (integer) that identifies a specific tweet\n","    # the value is the document embedding for that document\n","    ind2Doc_dict = {}\n","\n","    # this is list that will store the document vectors\n","    document_vec_l = []\n","\n","    for i, doc in enumerate(all_docs):\n","\n","        # get the document embedding of the tweet\n","        doc_embedding = get_document_embedding(doc,en_embeddings)\n","\n","        # save the document embedding into the ind2Tweet dictionary at index i\n","        ind2Doc_dict[i] = doc_embedding\n","\n","        # append the document embedding to the list of document vectors\n","        document_vec_l.append(doc_embedding)\n","\n","\n","    # convert the list of document vectors into a 2D array (each row is a document vector)\n","    document_vec_matrix = np.vstack(document_vec_l)\n","\n","    return document_vec_matrix, ind2Doc_dict"],"id":"2VyV5UAI8CxL"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0S07QJvK8CxL"},"outputs":[],"source":["document_vecs, ind2Tweet = get_document_vecs(all_tweets, en_embeddings_subset)\n"],"id":"0S07QJvK8CxL"},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"id":"Lap_DXGQ8CxM","outputId":"10436fb4-1972-470b-d8e0-66d8823a828c"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of dictionary 10000\n","shape of document_vecs (10000, 300)\n"]}],"source":["# UNQ_C15 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","print(f\"length of dictionary {len(ind2Tweet)}\")\n","print(f\"shape of document_vecs {document_vecs.shape}\")"],"id":"Lap_DXGQ8CxM"},{"cell_type":"markdown","metadata":{"id":"wyf-09A18CxM"},"source":["##### Expected Output\n","```\n","length of dictionary 10000\n","shape of document_vecs (10000, 300)\n","```"],"id":"wyf-09A18CxM"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6L5dPfN8CxM","executionInfo":{"status":"ok","timestamp":1715152910883,"user_tz":-120,"elapsed":39077,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"d40e51ea-bec3-4d48-9df8-097d46a77974"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m All tests passed\n"]}],"source":["#Â Test your function. This cell may take some seconds to run.\n","w4_unittest.test_get_document_vecs(get_document_vecs)"],"id":"o6L5dPfN8CxM"},{"cell_type":"markdown","metadata":{"id":"c8G8lDqU8CxM"},"source":["<a name=\"1-2\"></a>\n","## 2 - Looking up the Tweets\n","\n","Now you have a vector of dimension (m,d) where `m` is the number of tweets\n","(10,000) and `d` is the dimension of the embeddings (300).  Now you\n","will input a tweet, and use cosine similarity to see which tweet in our\n","corpus is similar to your tweet."],"id":"c8G8lDqU8CxM"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MeA6N9ep8CxM"},"outputs":[],"source":["my_tweet = 'i am sad'\n","process_tweet(my_tweet)\n","tweet_embedding = get_document_embedding(my_tweet, en_embeddings_subset)"],"id":"MeA6N9ep8CxM"},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"AXEiBXA38CxN","executionInfo":{"status":"ok","timestamp":1715152931581,"user_tz":-120,"elapsed":306,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"51fd6182-7eb6-4d73-e25e-c19791c85ea8"},"outputs":[{"output_type":"stream","name":"stdout","text":["@hanbined sad pray for me :(((\n"]}],"source":["# UNQ_C16 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","# this gives you a similar tweet as your input.\n","# this implementation is vectorized...\n","idx = np.argmax(cosine_similarity(document_vecs, tweet_embedding))\n","print(all_tweets[idx])"],"id":"AXEiBXA38CxN"},{"cell_type":"markdown","metadata":{"id":"cDDE-G0Y8CxN"},"source":["##### Expected Output\n","\n","```\n","@hanbined sad pray for me :(((\n","```"],"id":"cDDE-G0Y8CxN"},{"cell_type":"markdown","metadata":{"id":"02frQng48CxN"},"source":["<a name=\"1-3\"></a>\n","## 3 - Finding the most Similar Tweets with LSH\n","\n","You will now implement locality sensitive hashing (LSH) to identify the most similar tweet.\n","* Instead of looking at all 10,000 vectors, you can just search a subset to find\n","its nearest neighbors.\n","\n","Let's say your data points are plotted like this:\n","\n","\n","<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='./images/one.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 3 </div>\n","\n","You can divide the vector space into regions and search within one region for nearest neighbors of a given vector.\n","\n","<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='./images/four.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:400px;height:200px;\" /> Figure 4 </div>"],"id":"02frQng48CxN"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-XVBhD18CxN","executionInfo":{"status":"ok","timestamp":1715153035028,"user_tz":-120,"elapsed":249,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"ad0d7c48-bb0a-4790-f451-44f588c6e410"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of vectors is 10000 and each has 300 dimensions.\n"]}],"source":["N_VECS = len(all_tweets)       # This many vectors.\n","N_DIMS = len(ind2Tweet[1])     # Vector dimensionality.\n","print(f\"Number of vectors is {N_VECS} and each has {N_DIMS} dimensions.\")"],"id":"f-XVBhD18CxN"},{"cell_type":"markdown","metadata":{"id":"YJgeV8en8CxN"},"source":["#### Choosing the number of planes\n","\n","* Each plane divides the space to $2$ parts.\n","* So $n$ planes divide the space into $2^{n}$ hash buckets.\n","* We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ vectors.\n","* For that we need $\\frac{10000}{16}=625$ buckets.\n","* We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$."],"id":"YJgeV8en8CxN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"-bDmdR4N8CxO"},"outputs":[],"source":["# The number of planes. We use log2(625) to have ~16 vectors/bucket.\n","N_PLANES = 10\n","# Number of times to repeat the hashing to improve the search.\n","N_UNIVERSES = 25"],"id":"-bDmdR4N8CxO"},{"cell_type":"markdown","metadata":{"id":"DkfZFcH08CxO"},"source":["<a name=\"1-4\"></a>\n","## 4 - Getting the Hash Number for a Vector\n","\n","For each vector, we need to get a unique number associated to that vector in order to assign it to a \"hash bucket\".\n","\n","#### Hyperplanes in Vector Spaces\n","* In $3$-dimensional vector space, the hyperplane is a regular plane. In $2$ dimensional vector space, the hyperplane is a line.\n","* Generally, the hyperplane is subspace which has dimension $1$ lower than the original vector space has.\n","* A hyperplane is uniquely defined by its normal vector.\n","* Normal vector $n$ of the plane $\\pi$ is the vector to which all vectors in the plane $\\pi$ are orthogonal (perpendicular in $3$ dimensional case).\n","\n","#### Using Hyperplanes to Split the Vector Space\n","We can use a hyperplane to split the vector space into $2$ parts.\n","* All vectors whose dot product with a plane's normal vector is positive are on one side of the plane.\n","* All vectors whose dot product with the plane's normal vector is negative are on the other side of the plane.\n","\n","#### Encoding Hash Buckets\n","* For a vector, we can take its dot product with all the planes, then encode this information to assign the vector to a single hash bucket.\n","* When the vector is pointing to the opposite side of the hyperplane than normal, encode it by 0.\n","* Otherwise, if the vector is on the same side as the normal vector, encode it by 1.\n","* If you calculate the dot product with each plane in the same order for every vector, you've encoded each vector's unique hash ID as a binary number, like [0, 1, 1, ... 0]."],"id":"DkfZFcH08CxO"},{"cell_type":"markdown","metadata":{"id":"BFzzKUSM8CxO"},"source":["<a name=\"ex-9\"></a>\n","### hash_value_of_vector\n","\n","We've initialized hash table `hashes` for you. It is list of `N_UNIVERSES` matrices, each describes its own hash table. Each matrix has `N_DIMS` rows and `N_PLANES` columns. Every column of that matrix is a `N_DIMS`-dimensional normal vector for each of `N_PLANES` hyperplanes which are used for creating buckets of the particular hash table.\n","\n","*Exercise*: Your task is to complete the function `hash_value_of_vector` which places vector `v` in the correct hash bucket.\n","\n","* First multiply your vector `v`, with a corresponding plane. This will give you a vector of dimension $(1,\\text{N_planes})$.\n","* You will then convert every element in that vector to 0 or 1.\n","* You create a hash vector by doing the following: if the element is negative, it becomes a 0, otherwise you change it to a 1.\n","* You then compute the unique number for the vector by iterating over `N_PLANES`\n","* Then you multiply $2^i$ times the corresponding bit (0 or 1).\n","* You will then store that sum in the variable `hash_value`.\n","\n","**Intructions:** Create a hash for the vector in the function below.\n","Use this formula:\n","\n","$$ hash = \\sum_{i=0}^{N-1} \\left( 2^{i} \\times h_{i} \\right) $$"],"id":"BFzzKUSM8CxO"},{"cell_type":"markdown","metadata":{"id":"W0br2qqP8CxP"},"source":["#### Create the sets of planes\n","* Create multiple (25) sets of planes (the planes that divide up the region).\n","* You can think of these as 25 separate ways of dividing up the vector space with a different set of planes.\n","* Each element of this list contains a matrix with 300 rows (the word vector have 300 dimensions), and 10 columns (there are 10 planes in each \"universe\")."],"id":"W0br2qqP8CxP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L89-AHt8CxP"},"outputs":[],"source":["np.random.seed(0)\n","planes_l = [np.random.normal(size=(N_DIMS, N_PLANES))\n","            for _ in range(N_UNIVERSES)]"],"id":"7L89-AHt8CxP"},{"cell_type":"markdown","metadata":{"id":"_UtJQYQZ8CxP"},"source":["<details>\n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> numpy.squeeze() removes unused dimensions from an array; for instance, it converts a (10,1) 2D array into a (10,) 1D array</li>\n","</ul>\n","</p>"],"id":"_UtJQYQZ8CxP"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vWVfYZ38CxQ"},"outputs":[],"source":["# UNQ_C17 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","def hash_value_of_vector(v, planes):\n","    \"\"\"Create a hash for a vector; hash_id says which random hash to use.\n","    Input:\n","        - v:  vector of tweet. It's dimension is (1, N_DIMS)\n","        - planes: matrix of dimension (N_DIMS, N_PLANES) - the set of planes that divide up the region\n","    Output:\n","        - res: a number which is used as a hash for your vector\n","\n","    \"\"\"\n","    # for the set of planes,\n","    # calculate the dot product between the vector and the matrix containing the planes\n","    # remember that planes has shape (300, 10)\n","    # The dot product will have the shape (1,10)\n","    dot_product = np.dot(v,planes)\n","\n","    # get the sign of the dot product (1,10) shaped vector\n","    sign_of_dot_product = np.sign(dot_product)\n","\n","    # set h to be false (eqivalent to 0 when used in operations) if the sign is negative,\n","    # and true (equivalent to 1) if the sign is positive (1,10) shaped vector\n","    # if the sign is 0, i.e. the vector is in the plane, consider the sign to be positive\n","    h = (sign_of_dot_product>=0)\n","\n","    # remove extra un-used dimensions (convert this from a 2D to a 1D array)\n","    h = h.flatten()\n","\n","    # initialize the hash value to 0\n","    hash_value = 0\n","\n","    n_planes = planes.shape[1]\n","    for i in range(n_planes):\n","        # increment the hash value by 2^i * h_i\n","        hash_value += 2**i*h[i]\n","\n","\n","    # cast hash_value as an integer\n","    hash_value = int(hash_value)\n","\n","    return hash_value"],"id":"1vWVfYZ38CxQ"},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"xMMKmpgg8CxQ","executionInfo":{"status":"ok","timestamp":1715153064712,"user_tz":-120,"elapsed":295,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"538a2fba-e64e-4066-a362-7d4ce74b3315"},"outputs":[{"output_type":"stream","name":"stdout","text":[" The hash value for this vector, and the set of planes at index 0, is 768\n"]}],"source":["# UNQ_C18 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","np.random.seed(0)\n","idx = 0\n","planes = planes_l[idx]  # get one 'universe' of planes to test the function\n","vec = np.random.rand(1, 300)\n","print(f\" The hash value for this vector,\",\n","      f\"and the set of planes at index {idx},\",\n","      f\"is {hash_value_of_vector(vec, planes)}\")"],"id":"xMMKmpgg8CxQ"},{"cell_type":"markdown","metadata":{"id":"OQkmB4zE8CxQ"},"source":["##### Expected Output\n","\n","```\n","The hash value for this vector, and the set of planes at index 0, is 768\n","```"],"id":"OQkmB4zE8CxQ"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyNDs2EN8CxQ","executionInfo":{"status":"ok","timestamp":1715153075535,"user_tz":-120,"elapsed":311,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"7062096f-a9a9-4d7a-b720-517e95a86b93"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m All tests passed\n"]}],"source":["#Â Test your function\n","w4_unittest.test_hash_value_of_vector(hash_value_of_vector)"],"id":"dyNDs2EN8CxQ"},{"cell_type":"markdown","metadata":{"id":"ObeL2Qqb8CxR"},"source":["<a name=\"1-5\"></a>\n","## 5 - Creating a Hash Table\n","\n","<a name=\"ex-10\"></a>\n","###  make_hash_table\n","\n","Given that you have a unique number for each vector (or tweet), You now want to create a hash table. You need a hash table, so that given a hash_id, you can quickly look up the corresponding vectors. This allows you to reduce your search by a significant amount of time.\n","\n","<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='./images/table.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:500px;height:200px;\" />  </div>\n","\n","We have given you the `make_hash_table` function, which maps the tweet vectors to a bucket and stores the vector there. It returns the `hash_table` and the `id_table`. The `id_table` allows you know which vector in a certain bucket corresponds to what tweet."],"id":"ObeL2Qqb8CxR"},{"cell_type":"markdown","metadata":{"id":"0mNk9xLz8CxR"},"source":["<details>    \n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> a dictionary comprehension, similar to a list comprehension, looks like this: `{i:0 for i in range(10)}`, where the key is 'i' and the value is zero for all key-value pairs. </li>\n","</ul>\n","</p>"],"id":"0mNk9xLz8CxR"},{"cell_type":"code","execution_count":null,"metadata":{"deleteable":false,"editable":false,"id":"RtV-iB-s8CxS"},"outputs":[],"source":["# UNQ_C19 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","\n","# This is the code used to create a hash table:\n","# This function is already implemented for you. Feel free to read over it.\n","\n","\n","def make_hash_table(vecs, planes, hash_value_of_vector=hash_value_of_vector):\n","    \"\"\"\n","    Input:\n","        - vecs: list of vectors to be hashed.\n","        - planes: the matrix of planes in a single \"universe\", with shape (embedding dimensions, number of planes).\n","    Output:\n","        - hash_table: dictionary - keys are hashes, values are lists of vectors (hash buckets)\n","        - id_table: dictionary - keys are hashes, values are list of vectors id's\n","                            (it's used to know which tweet corresponds to the hashed vector)\n","    \"\"\"\n","    # number of planes is the number of columns in the planes matrix\n","    num_of_planes = planes.shape[1]\n","\n","    # number of buckets is 2^(number of planes)\n","    # ALTERNATIVE SOLUTION COMMENT:\n","    # num_buckets = pow(2, num_of_planes)\n","    num_buckets = 2**num_of_planes\n","\n","    # create the hash table as a dictionary.\n","    # Keys are integers (0,1,2.. number of buckets)\n","    # Values are empty lists\n","    hash_table = {i: [] for i in range(num_buckets)}\n","\n","    # create the id table as a dictionary.\n","    # Keys are integers (0,1,2... number of buckets)\n","    # Values are empty lists\n","    id_table = {i: [] for i in range(num_buckets)}\n","\n","    # for each vector in 'vecs'\n","    for i, v in enumerate(vecs):\n","        # calculate the hash value for the vector\n","        h = hash_value_of_vector(v, planes)\n","\n","        # store the vector into hash_table at key h,\n","        # by appending the vector v to the list at key h\n","        hash_table[h].append(v) # @REPLACE None\n","\n","        # store the vector's index 'i' (each document is given a unique integer 0,1,2...)\n","        # the key is the h, and the 'i' is appended to the list at key h\n","        id_table[h].append(i) # @REPLACE None\n","\n","    return hash_table, id_table"],"id":"RtV-iB-s8CxS"},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"2fd000PY8CxT","executionInfo":{"status":"ok","timestamp":1715153101346,"user_tz":-120,"elapsed":506,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"0a5f37fa-6709-4017-c6d9-3bb80a97618e"},"outputs":[{"output_type":"stream","name":"stdout","text":["The hash table at key 0 has 3 document vectors\n","The id table at key 0 has 3 document indices\n","The first 5 document indices stored at key 0 of id table are [3276, 3281, 3282]\n"]}],"source":["# UNQ_C20 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","planes = planes_l[0]  # get one 'universe' of planes to test the function\n","tmp_hash_table, tmp_id_table = make_hash_table(document_vecs, planes)\n","\n","print(f\"The hash table at key 0 has {len(tmp_hash_table[0])} document vectors\")\n","print(f\"The id table at key 0 has {len(tmp_id_table[0])} document indices\")\n","print(f\"The first 5 document indices stored at key 0 of id table are {tmp_id_table[0][0:5]}\")"],"id":"2fd000PY8CxT"},{"cell_type":"markdown","metadata":{"id":"6IMKGlYD8CxT"},"source":["##### Expected output\n","```\n","The hash table at key 0 has 3 document vectors\n","The id table at key 0 has 3 document indices\n","The first 5 document indices stored at key 0 of id table are [3276, 3281, 3282]\n","```"],"id":"6IMKGlYD8CxT"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I-BDMZEm8CxT","executionInfo":{"status":"ok","timestamp":1715153114831,"user_tz":-120,"elapsed":253,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"7e1911aa-6c32-4921-8514-fed5d2239f58"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m All tests passed\n"]}],"source":["# Test your function\n","w4_unittest.test_make_hash_table(make_hash_table)"],"id":"I-BDMZEm8CxT"},{"cell_type":"markdown","metadata":{"id":"CPknUEp88CxU"},"source":["<a name=\"1-6\"></a>\n","## 6 - Creating all Hash Tables\n","\n","You can now hash your vectors and store them in a hash table that\n","would allow you to quickly look up and search for similar vectors.\n","Run the cell below to create the hashes. By doing so, you end up having\n","several tables which have all the vectors. Given a vector, you then\n","identify the buckets in all the tables.  You can then iterate over the\n","buckets and consider much fewer vectors. The more tables you use, the\n","more accurate your lookup will be, but also the longer it will take."],"id":"CPknUEp88CxU"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x0tYrHwm8CxU","executionInfo":{"status":"ok","timestamp":1715153126694,"user_tz":-120,"elapsed":7944,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"67f4ed12-a179-43fb-fd14-3dbabfaed1a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["working on hash universe #: 0\n","working on hash universe #: 1\n","working on hash universe #: 2\n","working on hash universe #: 3\n","working on hash universe #: 4\n","working on hash universe #: 5\n","working on hash universe #: 6\n","working on hash universe #: 7\n","working on hash universe #: 8\n","working on hash universe #: 9\n","working on hash universe #: 10\n","working on hash universe #: 11\n","working on hash universe #: 12\n","working on hash universe #: 13\n","working on hash universe #: 14\n","working on hash universe #: 15\n","working on hash universe #: 16\n","working on hash universe #: 17\n","working on hash universe #: 18\n","working on hash universe #: 19\n","working on hash universe #: 20\n","working on hash universe #: 21\n","working on hash universe #: 22\n","working on hash universe #: 23\n","working on hash universe #: 24\n"]}],"source":["# Creating the hashtables\n","def create_hash_id_tables(n_universes):\n","    hash_tables = []\n","    id_tables = []\n","    for universe_id in range(n_universes):  # there are 25 hashes\n","        print('working on hash universe #:', universe_id)\n","        planes = planes_l[universe_id]\n","        hash_table, id_table = make_hash_table(document_vecs, planes)\n","        hash_tables.append(hash_table)\n","        id_tables.append(id_table)\n","\n","    return hash_tables, id_tables\n","\n","hash_tables, id_tables = create_hash_id_tables(N_UNIVERSES)"],"id":"x0tYrHwm8CxU"},{"cell_type":"markdown","metadata":{"id":"JyNesqQF8CxV"},"source":["#### Approximate K-NN\n","\n","<a name=\"ex-11\"></a>\n","### approximate_knn\n","\n","Implement approximate K nearest neighbors using locality sensitive hashing,\n","to search for documents that are similar to a given document at the\n","index `doc_id`.\n","\n","##### Inputs\n","* `doc_id` is the index into the document list `all_tweets`.\n","* `v` is the document vector for the tweet in `all_tweets` at index `doc_id`.\n","* `planes_l` is the list of planes (the global variable created earlier).\n","* `k` is the number of nearest neighbors to search for.\n","* `num_universes_to_use`: to save time, we can use fewer than the total\n","number of available universes.  By default, it's set to `N_UNIVERSES`,\n","which is $25$ for this assignment.\n","* `hash_tables`: list with hash tables for each universe.\n","* `id_tables`: list with id tables for each universe.\n","\n","The `approximate_knn` function finds a subset of candidate vectors that\n","are in the same \"hash bucket\" as the input vector 'v'.  Then it performs\n","the usual k-nearest neighbors search on this subset (instead of searching\n","through all 10,000 tweets)."],"id":"JyNesqQF8CxV"},{"cell_type":"markdown","metadata":{"id":"Fnj6aQ--8CxV"},"source":["<details>\n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> There are many dictionaries used in this function.  Try to print out planes_l, hash_tables, id_tables to understand how they are structured, what the keys represent, and what the values contain.</li>\n","    <li> To remove an item from a list, use `.remove()` </li>\n","    <li> To append to a list, use `.append()` </li>\n","    <li> To add to a set, use `.add()` </li>\n","</ul>\n","</p>"],"id":"Fnj6aQ--8CxV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"I7ycy7ZK8CxV"},"outputs":[],"source":["# UNQ_C21 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# This is the code used to do the fast nearest neighbor search. Feel free to go over it\n","def approximate_knn(doc_id, v, planes_l, hash_tables, id_tables, k=1, num_universes_to_use=25, hash_value_of_vector=hash_value_of_vector):\n","    \"\"\"Search for k-NN using hashes.\"\"\"\n","    #assert num_universes_to_use <= N_UNIVERSES\n","\n","    # Vectors that will be checked as possible nearest neighbor\n","    vecs_to_consider_l = list()\n","\n","    # list of document IDs\n","    ids_to_consider_l = list()\n","\n","    # create a set for ids to consider, for faster checking if a document ID already exists in the set\n","    ids_to_consider_set = set()\n","\n","    # loop through the universes of planes\n","    for universe_id in range(num_universes_to_use):\n","\n","        # get the set of planes from the planes_l list, for this particular universe_id\n","        planes = planes_l[universe_id]\n","\n","        # get the hash value of the vector for this set of planes\n","        hash_value = hash_value_of_vector(v, planes)\n","\n","        # get the hash table for this particular universe_id\n","        hash_table = hash_tables[universe_id]\n","\n","        # get the list of document vectors for this hash table, where the key is the hash_value\n","        document_vectors_l = hash_table[hash_value]\n","\n","        # get the id_table for this particular universe_id\n","        id_table = id_tables[universe_id]\n","\n","        # get the subset of documents to consider as nearest neighbors from this id_table dictionary\n","        new_ids_to_consider = id_table[hash_value]\n","\n","        # loop through the subset of document vectors to consider\n","        for i, new_id in enumerate(new_ids_to_consider):\n","\n","            if doc_id == new_id:\n","                continue\n","\n","            # if the document ID is not yet in the set ids_to_consider...\n","            if new_id not in ids_to_consider_set:\n","                # access document_vectors_l list at index i to get the embedding\n","                # then append it to the list of vectors to consider as possible nearest neighbors\n","                document_vector_at_i = document_vectors_l[i]\n","                vecs_to_consider_l.append(document_vector_at_i)\n","\n","                # append the new_id (the index for the document) to the list of ids to consider\n","                ids_to_consider_l.append(new_id)\n","\n","                # also add the new_id to the set of ids to consider\n","                # (use this to check if new_id is not already in the IDs to consider)\n","                ids_to_consider_set.add(new_id)\n","\n","\n","    # Now run k-NN on the smaller set of vecs-to-consider.\n","    print(\"Fast considering %d vecs\" % len(vecs_to_consider_l))\n","\n","    # convert the vecs to consider set to a list, then to a numpy array\n","    vecs_to_consider_arr = np.array(vecs_to_consider_l)\n","\n","    # call nearest neighbors on the reduced list of candidate vectors\n","    nearest_neighbor_idx_l = nearest_neighbor(v, vecs_to_consider_arr, k=k)\n","\n","    # Use the nearest neighbor index list as indices into the ids to consider\n","    # create a list of nearest neighbors by the document ids\n","    nearest_neighbor_ids = [ids_to_consider_l[idx]\n","                            for idx in nearest_neighbor_idx_l]\n","\n","    return nearest_neighbor_ids"],"id":"I7ycy7ZK8CxV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaVsmXpE8CxW"},"outputs":[],"source":["#document_vecs, ind2Tweet\n","doc_id = 0\n","doc_to_search = all_tweets[doc_id]\n","vec_to_search = document_vecs[doc_id]"],"id":"YaVsmXpE8CxW"},{"cell_type":"markdown","source":["<a name=\"ex-5\"></a>\n","###   nearest_neighbor\n","Complete the function `nearest_neighbor()`\n","\n","Inputs:\n","* Vector `v`,\n","* A set of possible nearest neighbors `candidates`\n","* `k` nearest neighbors to find.\n","* The distance metric should be based on cosine similarity.\n","* `cosine_similarity` function is already implemented and imported for you. It's arguments are two vectors and it returns the cosine of the angle between them.\n","* Iterate over rows in `candidates`, and save the result of similarities between current row and vector `v` in a python list. Take care that similarities are in the same order as row vectors of `candidates`.\n","* Now you can use [numpy argsort]( https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html#numpy.argsort) to sort the indices for the rows of `candidates`."],"metadata":{"id":"lnyLsJCdLnEJ"},"id":"lnyLsJCdLnEJ"},{"cell_type":"markdown","source":["<details>\n","<summary>\n","    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n","</summary>\n","<p>\n","<ul>\n","    <li> numpy.argsort sorts values from most negative to most positive (smallest to largest) </li>\n","    <li> The candidates that are nearest to 'v' should have the highest cosine similarity </li>\n","    <li> To reverse the order of the result of numpy.argsort to get the element with highest cosine similarity as the first element of the array you can use tmp[::-1]. This reverses the order of an array. Then, you can extract the first k elements.</li>\n","</ul>\n","</p>"],"metadata":{"id":"AIhl2_05Lzvp"},"id":"AIhl2_05Lzvp"},{"cell_type":"code","source":["# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","def nearest_neighbor(v, candidates, k=1, cosine_similarity=cosine_similarity):\n","    \"\"\"\n","    Input:\n","      - v, the vector you are going find the nearest neighbor for\n","      - candidates: a set of vectors where we will find the neighbors\n","      - k: top k nearest neighbors to find\n","    Output:\n","      - k_idx: the indices of the top k closest vectors in sorted form\n","    \"\"\"\n","\n","    similarity_l = []\n","\n","    # for each candidate vector...\n","    for row in candidates:\n","        # get the cosine similarity\n","        cos_similarity = cosine_similarity(v,row)\n","\n","        # append the similarity to the list\n","        similarity_l.append(cos_similarity)\n","\n","    # sort the similarity list and get the indices of the sorted list\n","    sorted_ids = np.argsort(similarity_l)\n","\n","    # Reverse the order of the sorted_ids array\n","    sorted_ids = sorted_ids[::-1]\n","\n","    # get the indices of the k most similar candidate vectors\n","    k_idx = sorted_ids[:k]\n","\n","    return k_idx"],"metadata":{"id":"WoAPEQX0L5iI"},"id":"WoAPEQX0L5iI","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# UNQ_C9 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","# Test your implementation:\n","v = np.array([1, 0, 1])\n","candidates = np.array([[1, 0, 5], [-2, 5, 3], [2, 0, 1], [6, -9, 5], [9, 9, 9]])\n","print(candidates[nearest_neighbor(v, candidates, 3)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKrteY9iL-wz","executionInfo":{"status":"ok","timestamp":1715154797720,"user_tz":-120,"elapsed":250,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"f0dc2820-f3d1-45f3-c5bd-760dfdc0c022"},"id":"yKrteY9iL-wz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2 0 1]\n"," [1 0 5]\n"," [9 9 9]]\n"]}]},{"cell_type":"markdown","source":["**Expected Output**:\n","\n","`[[2 0 1]\n"," [1 0 5]\n"," [9 9 9]]`"],"metadata":{"id":"jValNDY8L_oL"},"id":"jValNDY8L_oL"},{"cell_type":"code","source":["# Test your function\n","w4_unittest.test_nearest_neighbor(nearest_neighbor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jYNOIbPKMJqN","executionInfo":{"status":"ok","timestamp":1715154803669,"user_tz":-120,"elapsed":790,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"6886df36-435b-4a4e-eca5-e42d3f76e8d1"},"id":"jYNOIbPKMJqN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[92m All tests passed\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"naWaGbXA8CxW","executionInfo":{"status":"ok","timestamp":1715154807285,"user_tz":-120,"elapsed":230,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"c0d01e11-8f54-4f81-fb98-71332b40c871"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fast considering 77 vecs\n"]}],"source":["# UNQ_C22 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# You do not have to input any code in this cell, but it is relevant to grading, so please do not change anything\n","\n","# Sample\n","nearest_neighbor_ids = approximate_knn(\n","    doc_id, vec_to_search, planes_l, hash_tables, id_tables, k=3, num_universes_to_use=5)"],"id":"naWaGbXA8CxW"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51w0XjjK8CxX","executionInfo":{"status":"ok","timestamp":1715154819200,"user_tz":-120,"elapsed":248,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"35bcba0f-eea7-4c12-c045-c63cc55254f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Nearest neighbors for document 0\n","Document contents: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","\n","Nearest neighbor at document id 51\n","document contents: #FollowFriday @France_Espana @reglisse_menthe @CCI_inter for being top engaged members in my community this week :)\n","Nearest neighbor at document id 2478\n","document contents: #ShareTheLove @oymgroup @musicartisthere for being top HighValue members this week :) @nataliavas http://t.co/IWSDMtcayt\n","Nearest neighbor at document id 105\n","document contents: #FollowFriday @straz_das @DCarsonCPA @GH813600 for being top engaged members in my community this week :)\n"]}],"source":["print(f\"Nearest neighbors for document {doc_id}\")\n","print(f\"Document contents: {doc_to_search}\")\n","print(\"\")\n","\n","for neighbor_id in nearest_neighbor_ids:\n","    print(f\"Nearest neighbor at document id {neighbor_id}\")\n","    print(f\"document contents: {all_tweets[neighbor_id]}\")"],"id":"51w0XjjK8CxX"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"52nkZpSj8CxX","executionInfo":{"status":"ok","timestamp":1715154839092,"user_tz":-120,"elapsed":267,"user":{"displayName":"Fakhri Momeni","userId":"05371720958134709651"}},"outputId":"7666e4df-8785-47b6-f099-af5a503f25f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fast considering 77 vecs\n","Fast considering 153 vecs\n","\u001b[92m All tests passed\n"]}],"source":["#Â Test your function\n","w4_unittest.test_approximate_knn(approximate_knn, hash_tables, id_tables)"],"id":"52nkZpSj8CxX"},{"cell_type":"markdown","metadata":{"id":"WLwkcNGL8CxY"},"source":["#  Conclusion\n","Congratulations - Now you can look up vectors that are similar to the\n","encoding of your tweet using LSH!"],"id":"WLwkcNGL8CxY"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZtRKLfuF8CxY"},"outputs":[],"source":[],"id":"ZtRKLfuF8CxY"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}