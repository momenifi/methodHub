{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cbe4d6-0f25-44a4-bb58-b1136e1a3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base code: https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85962248-86f9-4d05-9b17-65a34826e279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install pandas\n",
    "#! pip install scikit-learn\n",
    "#! pip install keras\n",
    "#! pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "259e713e-f095-4861-b35e-d3598873cb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-29 03:30:16.155732: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-29 03:30:16.158697: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-06-29 03:30:16.195349: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-29 03:30:16.845612: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import keras\n",
    "import pandas as pd\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import emoji\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5147dd-e888-4966-82ac-0373d6f909a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('{\"id\": \"a733e8a47708ce1d77060266d365e5b5\", \"text\": \"Wen man nicht reinl\\\\u00e4\\\\u00dft, den muss man a',\n",
       " '{\"id\": \"f3b81af2f6852bf1b9896629525d2f41\", \"text\": \"Ja, Frauen k\\\\u00f6nnen krankhaft eifers\\\\u00fccht')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trial train data\n",
    "train_filepath = 'data/challenge_data/germeval-competition-traindev.jsonl' #'data/development_data/germeval-development-train.jsonl' \n",
    "test_filepath = 'data/challenge_data/germeval-competition-test.jsonl'  #'data/development_data/germeval-development-test.jsonl'\n",
    "train_data = open(train_filepath).read()\n",
    "test_data = open(test_filepath).read()\n",
    "train_data[:100], test_data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9557dcb-8a6d-42d3-9c8a-8f078044e6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 'a733e8a47708ce1d77060266d365e5b5',\n",
       "  'text': 'Wen man nicht reinläßt, den muss man auch nicht integrieren.',\n",
       "  'annotations': [{'user': 'A001', 'label': '0-Kein'},\n",
       "   {'user': 'A002', 'label': '0-Kein'},\n",
       "   {'user': 'A005', 'label': '0-Kein'},\n",
       "   {'user': 'A008', 'label': '0-Kein'},\n",
       "   {'user': 'A007', 'label': '0-Kein'},\n",
       "   {'user': 'A004', 'label': '0-Kein'},\n",
       "   {'user': 'A009', 'label': '0-Kein'},\n",
       "   {'user': 'A003', 'label': '0-Kein'},\n",
       "   {'user': 'A012', 'label': '0-Kein'},\n",
       "   {'user': 'A010', 'label': '0-Kein'}]},\n",
       " {'id': 'f3b81af2f6852bf1b9896629525d2f41',\n",
       "  'text': 'Ja, Frauen können krankhaft eifersüchtig werden.\\nDa haben Sie wohl Recht.\\n\\nZumeist ist dann aber Gift das Mittel der Wahl.',\n",
       "  'annotators': ['A001',\n",
       "   'A002',\n",
       "   'A005',\n",
       "   'A008',\n",
       "   'A007',\n",
       "   'A004',\n",
       "   'A009',\n",
       "   'A003',\n",
       "   'A010',\n",
       "   'A012']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json.loads(train_data)\n",
    "json_train_data = [json.loads(jline) for jline in train_data.splitlines()]\n",
    "json_test_data = [json.loads(jline) for jline in test_data.splitlines()]\n",
    "\n",
    "numTrainSamples = len(json_train_data)\n",
    "json_train_data[0], json_test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64dea8e1-aba0-4cf5-a06e-2ba4a6841747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5998, 1986)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_train_data), len(json_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f298b2-dc17-4908-84e8-a19cf0030c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>annotators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7979</th>\n",
       "      <td>2f7322c62b63ff74ec945bb38ed9f258</td>\n",
       "      <td>Ihre Partnerin schämt sich wahrscheinlich für ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A002, A009, A010]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7980</th>\n",
       "      <td>ec5fe35f542aac2f3155177dbf2731c2</td>\n",
       "      <td>glaube ich diese These einfach nicht.Grund: He...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A002, A009, A010]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>6674986a02bab67b011df90cc7396a96</td>\n",
       "      <td>Damit die Ehefrau dann ein Dauervisum erhält m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A002, A009, A010]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7982</th>\n",
       "      <td>2a3774eba33afe18af2f0d312d081bb3</td>\n",
       "      <td>Ich selbst habe in meiner Hierarchie zwei Frau...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A002, A009, A010]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7983</th>\n",
       "      <td>43891e25f522166dda98045a33563c05</td>\n",
       "      <td>Ich finde die Nahaufnahmen  und sexistische Be...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[A002, A009, A010]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id  \\\n",
       "7979  2f7322c62b63ff74ec945bb38ed9f258   \n",
       "7980  ec5fe35f542aac2f3155177dbf2731c2   \n",
       "7981  6674986a02bab67b011df90cc7396a96   \n",
       "7982  2a3774eba33afe18af2f0d312d081bb3   \n",
       "7983  43891e25f522166dda98045a33563c05   \n",
       "\n",
       "                                                   text annotations  \\\n",
       "7979  Ihre Partnerin schämt sich wahrscheinlich für ...         NaN   \n",
       "7980  glaube ich diese These einfach nicht.Grund: He...         NaN   \n",
       "7981  Damit die Ehefrau dann ein Dauervisum erhält m...         NaN   \n",
       "7982  Ich selbst habe in meiner Hierarchie zwei Frau...         NaN   \n",
       "7983  Ich finde die Nahaufnahmen  und sexistische Be...         NaN   \n",
       "\n",
       "              annotators  \n",
       "7979  [A002, A009, A010]  \n",
       "7980  [A002, A009, A010]  \n",
       "7981  [A002, A009, A010]  \n",
       "7982  [A002, A009, A010]  \n",
       "7983  [A002, A009, A010]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(json_train_data+json_test_data)\n",
    "#df.head()\n",
    "X = list(df['text'])\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b08b9ed-2841-431b-a169-87678bdd3395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A001',\n",
       " 'A002',\n",
       " 'A003',\n",
       " 'A004',\n",
       " 'A005',\n",
       " 'A007',\n",
       " 'A008',\n",
       " 'A009',\n",
       " 'A010',\n",
       " 'A012']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the list of all annotator IDs\n",
    "#annotator_ids = sorted(set([ann['user'] for annotations in df['annotations'] for ann in annotations]))\n",
    "\n",
    "\n",
    "# Define the function to extract annotator IDs\n",
    "def get_annotator_ids(train_filepath, test_filepath):\n",
    "    train_data = open(train_filepath).read()\n",
    "    json_train_data = [json.loads(jline) for jline in train_data.splitlines()]\n",
    "    train_annotator_ids = sorted(set([ann['user'] for annotations in json_train_data for ann in annotations['annotations']]))\n",
    "\n",
    "    test_data = open(test_filepath).read()\n",
    "    json_test_data = [json.loads(jline) for jline in test_data.splitlines()]\n",
    "    test_annotator_ids = sorted(set([annotator for annotations in json_test_data for annotator in annotations['annotators']]))\n",
    "\n",
    "    # Combine and deduplicate annotator IDs from both train and test data\n",
    "    annotator_ids = sorted(set(train_annotator_ids + test_annotator_ids))\n",
    "\n",
    "    return annotator_ids\n",
    "\n",
    "annotator_ids = get_annotator_ids(train_filepath, test_filepath)\n",
    "annotator_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53126692-47ae-4039-bef3-624c4e4ad28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extraction functions\n",
    "def count_special_characters(text):\n",
    "    return len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', text))\n",
    "\n",
    "def count_capitalized_words(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def count_emojis(text):\n",
    "    return len(emoji.emoji_list(text))\n",
    "\n",
    "def count_characters(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_punctuations(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "# Define the function to process data\n",
    "def get_additional_features(filepath, annotator_ids,annotations_key):\n",
    "    # Load data\n",
    "    data = open(filepath).read()\n",
    "    json_data = [json.loads(jline) for jline in data.splitlines()]\n",
    "    df = pd.DataFrame(json_data)\n",
    "    X = list(df['text'])\n",
    "\n",
    "    # Initialize a list to store binary vectors for each document\n",
    "    annotator_presence = []\n",
    "\n",
    "    # Initialize a list to store binary vectors for each document\n",
    "    annotator_presence = []\n",
    "\n",
    "    # Iterate over the annotations for each document\n",
    "    for annotations in df[annotations_key]:\n",
    "        # Initialize a binary vector for the current document\n",
    "        doc_vector = [0] * len(annotator_ids)\n",
    "        # Mark the presence of each annotator\n",
    "        for ann in annotations:\n",
    "            user = ann if annotations_key == 'annotators' else ann['user']\n",
    "            idx = annotator_ids.index(user)\n",
    "            doc_vector[idx] = 1\n",
    "        # Append the binary vector to the list\n",
    "        annotator_presence.append(doc_vector)\n",
    "\n",
    "    # Convert the list of binary vectors into a numpy array\n",
    "    annotator_presence_matrix = np.array(annotator_presence)\n",
    "\n",
    "    # Apply the feature extraction functions to your dataset\n",
    "    df['special_char_count'] = df['text'].apply(count_special_characters)\n",
    "    df['capitalized_word_count'] = df['text'].apply(count_capitalized_words)\n",
    "#    df['emoji_count'] = df['text'].apply(count_emojis)\n",
    "    df['char_count'] = df['text'].apply(count_characters)\n",
    "    df['word_count'] = df['text'].apply(count_words)\n",
    "    df['punctuation_count'] = df['text'].apply(count_punctuations)\n",
    "\n",
    "    # Convert these features to numpy arrays\n",
    "    special_char_count = np.array(df['special_char_count']).reshape(-1, 1)\n",
    "    capitalized_word_count = np.array(df['capitalized_word_count']).reshape(-1, 1)\n",
    "#    emoji_count = np.array(df['emoji_count']).reshape(-1, 1)\n",
    "    char_count = np.array(df['char_count']).reshape(-1, 1)\n",
    "    word_count = np.array(df['word_count']).reshape(-1, 1)\n",
    "    punctuation_count = np.array(df['punctuation_count']).reshape(-1, 1)\n",
    "    \n",
    "    # Combine these features into a single array\n",
    "    additional_features = np.hstack([annotator_presence_matrix, special_char_count, capitalized_word_count, #emoji_count,\n",
    "                                     char_count, word_count, punctuation_count])\n",
    "\n",
    "    return additional_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60dbadc4-0e23-4101-a87b-12dad6f58fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "#train_filepath = 'data/development_data/germeval-development-train.jsonl'\n",
    "#test_filepath = 'data/development_data/germeval-development-test.jsonl'\n",
    "# Process training data\n",
    "additional_features = get_additional_features(train_filepath, annotator_ids, 'annotations')\n",
    "additional_features_test = get_additional_features(test_filepath, annotator_ids, 'annotators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04400b23-41b0-48ab-9e1e-3fa8e8805f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#defining additional features\\ndef count_special_characters(text):\\n    return len(re.findall(r\\'[!@#$%^&*(),.?\":{}|<>]\\', text))\\n\\ndef count_capitalized_words(text):\\n    return sum(1 for word in text.split() if word.isupper())\\n\\ndef count_emojis(text):\\n    return len(emoji.emoji_list(text))\\n\\ndef count_characters(text):\\n    return len(text)\\n\\ndef count_words(text):\\n    return len(text.split())\\n\\ndef count_punctuations(text):\\n    return sum(1 for char in text if char in string.punctuation)\\n\\n\\n# Apply the feature extraction functions to your dataset\\ndf[\\'special_char_count\\'] = df[\\'text\\'].apply(count_special_characters)\\ndf[\\'capitalized_word_count\\'] = df[\\'text\\'].apply(count_capitalized_words)\\ndf[\\'emoji_count\\'] = df[\\'text\\'].apply(count_emojis)\\ndf[\\'char_count\\'] = df[\\'text\\'].apply(count_characters)\\ndf[\\'word_count\\'] = df[\\'text\\'].apply(count_words)\\ndf[\\'punctuation_count\\'] = df[\\'text\\'].apply(count_punctuations)\\n\\n# Convert these features to numpy arrays\\nspecial_char_count = np.array(df[\\'special_char_count\\']).reshape(-1, 1)\\ncapitalized_word_count = np.array(df[\\'capitalized_word_count\\']).reshape(-1, 1)\\nemoji_count = np.array(df[\\'emoji_count\\']).reshape(-1, 1)\\nchar_count = np.array(df[\\'char_count\\']).reshape(-1, 1)\\nword_count = np.array(df[\\'word_count\\']).reshape(-1, 1)\\npunctuation_count = np.array(df[\\'punctuation_count\\']).reshape(-1, 1)\\n\\n# Combine these features into a single array\\n#additional_features = np.hstack([annotator_presence_matrix,special_char_count, capitalized_word_count, emoji_count,\\n#                                 char_count, word_count, punctuation_count])\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#defining additional features\n",
    "def count_special_characters(text):\n",
    "    return len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', text))\n",
    "\n",
    "def count_capitalized_words(text):\n",
    "    return sum(1 for word in text.split() if word.isupper())\n",
    "\n",
    "def count_emojis(text):\n",
    "    return len(emoji.emoji_list(text))\n",
    "\n",
    "def count_characters(text):\n",
    "    return len(text)\n",
    "\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "\n",
    "def count_punctuations(text):\n",
    "    return sum(1 for char in text if char in string.punctuation)\n",
    "\n",
    "\n",
    "# Apply the feature extraction functions to your dataset\n",
    "df['special_char_count'] = df['text'].apply(count_special_characters)\n",
    "df['capitalized_word_count'] = df['text'].apply(count_capitalized_words)\n",
    "df['emoji_count'] = df['text'].apply(count_emojis)\n",
    "df['char_count'] = df['text'].apply(count_characters)\n",
    "df['word_count'] = df['text'].apply(count_words)\n",
    "df['punctuation_count'] = df['text'].apply(count_punctuations)\n",
    "\n",
    "# Convert these features to numpy arrays\n",
    "special_char_count = np.array(df['special_char_count']).reshape(-1, 1)\n",
    "capitalized_word_count = np.array(df['capitalized_word_count']).reshape(-1, 1)\n",
    "emoji_count = np.array(df['emoji_count']).reshape(-1, 1)\n",
    "char_count = np.array(df['char_count']).reshape(-1, 1)\n",
    "word_count = np.array(df['word_count']).reshape(-1, 1)\n",
    "punctuation_count = np.array(df['punctuation_count']).reshape(-1, 1)\n",
    "\n",
    "# Combine these features into a single array\n",
    "#additional_features = np.hstack([annotator_presence_matrix,special_char_count, capitalized_word_count, emoji_count,\n",
    "#                                 char_count, word_count, punctuation_count])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cdce88f-cce6-49e6-953f-5e4bbec06315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = [0 if label == 0 else 1 for label in labels]\n",
    "#y_train[:13]\n",
    "# Initialize label lists\n",
    "dictlabels = {'bin_maj': [], 'bin_one': [], 'bin_all': [], 'multi_maj': [], 'disagree_bin': [], 'dist_bin_0': [], 'dist_bin_1': [], 'dist_multi_0': [],\n",
    "            'dist_multi_1': [], 'dist_multi_2': [], 'dist_multi_3': [], 'dist_multi_4': []}\n",
    "\n",
    "for annotations in df['annotations'][:numTrainSamples]:\n",
    "    #print(annotations)\n",
    "    labels = [ann['label'] for ann in annotations]\n",
    "    # Count occurrences of each label\n",
    "    label_count = Counter(labels)\n",
    "    # Calculate the total number of annotations\n",
    "    total_annotations = sum(label_count.values())\n",
    "    # bin_maj\n",
    "    if label_count['0-Kein'] > sum(label_count.values()) / 2: #TK: Why /2? label_count.values() should be without label_count['0-kein'], use >= for one\n",
    "        dictlabels['bin_maj'].append(0)\n",
    "    else:\n",
    "        dictlabels['bin_maj'].append(1)  # If there was no majority, consider both 1 and 0 as correct\n",
    "\n",
    "    # bin_one\n",
    "    if any(label != '0-Kein' for label in labels): #TK: bin_one and bin_all - the code seems similar\n",
    "        dictlabels['bin_one'].append(1)\n",
    "    else:\n",
    "        dictlabels['bin_one'].append(0)\n",
    "\n",
    "    # bin_all\n",
    "    if all(label != '0-Kein' for label in labels):\n",
    "        dictlabels['bin_all'].append(1)\n",
    "    else:\n",
    "        dictlabels['bin_all'].append(0)\n",
    "\n",
    "    # multi_maj\n",
    "    #dictlabels['multi_maj'].append(label_count.most_common(1)[0][0][0])  # Append a list with a single integer\n",
    "    # multi_maj\n",
    "    dict_label_count = {'0-Kein': labels.count('0-Kein'), '1-Gering': labels.count('1-Gering'), \n",
    "                        '2-Vorhanden': labels.count('2-Vorhanden'), '3-Stark': labels.count('3-Stark'), '4-Extrem': labels.count('4-Extrem')}\n",
    "    maxVal = 0\n",
    "    lbl = ''\n",
    "    for k,v in dict_label_count.items():\n",
    "        if v >= maxVal:\n",
    "            maxVal = v\n",
    "            lbl = k\n",
    "    if lbl == '0-Kein':\n",
    "        dictlabels['multi_maj'].append([1, 0, 0, 0, 0]) \n",
    "    elif lbl == '1-Gering':\n",
    "        dictlabels['multi_maj'].append([0, 1, 0, 0, 0])\n",
    "    elif lbl == '2-Vorhanden':\n",
    "        dictlabels['multi_maj'].append([0, 0, 1, 0, 0])\n",
    "    elif lbl == '3-Stark':\n",
    "        dictlabels['multi_maj'].append([0, 0, 0, 1, 0])\n",
    "    elif lbl == '4-Extrem':\n",
    "        dictlabels['multi_maj'].append([0, 0, 0, 0, 1])\n",
    "\n",
    "    # disagree_bin\n",
    "    if len(label_count) > 1 and '0-Kein' in label_count:\n",
    "        dictlabels['disagree_bin'].append(1)\n",
    "    else:\n",
    "        dictlabels['disagree_bin'].append(0)\n",
    "\n",
    "    # Calculate dist_bin\n",
    "    dictlabels['dist_bin_0'].append(label_count['0-Kein'] / total_annotations)\n",
    "    dictlabels['dist_bin_1'].append((total_annotations - label_count['0-Kein']) / total_annotations)\n",
    "    \n",
    "    # Calculate dist_multi\n",
    "    dictlabels['dist_multi_0'].append(label_count['0-Kein'] / total_annotations)\n",
    "    dictlabels['dist_multi_1'].append(label_count['1-Gering'] / total_annotations)\n",
    "    dictlabels['dist_multi_2'].append(label_count['2-Vorhanden'] / total_annotations)\n",
    "    dictlabels['dist_multi_3'].append(label_count['3-Stark'] / total_annotations)\n",
    "    dictlabels['dist_multi_4'].append(label_count['4-Extrem'] / total_annotations) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4f0c8f8-1131-44ae-bac6-ed9a08aec910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_multi_maj = np.array([multi_maj_mapping[label[0]] if isinstance(label, list) else multi_maj_mapping[label] for label in df['multi_maj']])\n",
    "\n",
    "# Extract the new target variables as arrays\n",
    "y_bin_maj = np.array(dictlabels['bin_maj'])\n",
    "y_bin_one = np.array(dictlabels['bin_one'])\n",
    "y_bin_all = np.array(dictlabels['bin_all'])\n",
    "y_multi_maj = np.array(dictlabels['multi_maj'])\n",
    "y_disagree_bin = np.array(dictlabels['disagree_bin'])\n",
    "y_dist_bin_0 = np.array(dictlabels['dist_bin_0'])\n",
    "y_dist_bin_1 = np.array(dictlabels['dist_bin_1'])\n",
    "y_dist_multi_0 = np.array(dictlabels['dist_multi_0'])\n",
    "y_dist_multi_1 = np.array(dictlabels['dist_multi_1'])\n",
    "y_dist_multi_2 = np.array(dictlabels['dist_multi_2'])\n",
    "y_dist_multi_3 = np.array(dictlabels['dist_multi_3'])\n",
    "y_dist_multi_4 = np.array(dictlabels['dist_multi_4'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d379d4e8-e9e0-4d58-8cea-2a538722a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_multi_maj[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d598f77-cd4a-4dae-b558-c4055475d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the target variables into a single array if needed\n",
    "y_train_combined = np.column_stack([y_bin_maj, y_bin_one, y_bin_all, y_multi_maj, y_disagree_bin,\n",
    "                                    y_dist_bin_0, y_dist_bin_1, y_dist_multi_0, y_dist_multi_1, y_dist_multi_2, y_dist_multi_3, y_dist_multi_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c24c9f3-8f39-4d16-ab66-1d3aa3eee208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin_maj (0, 1):  65 %,  34 %\n",
      "bin_one (0, 1):  46 %,  53 %\n",
      "bin_all (0, 1):  83 %,  16 %\n",
      "multi_maj %:  67 3 15 10 1\n"
     ]
    }
   ],
   "source": [
    "#Output label distribution analysis\n",
    "print('bin_maj (0, 1): ', (y_bin_maj == 0).sum()*100//len(y_bin_maj),'%, ', (y_bin_maj == 1).sum()*100//len(y_bin_maj), '%')\n",
    "print('bin_one (0, 1): ', (y_bin_one == 0).sum()*100//len(y_bin_one),'%, ', (y_bin_one == 1).sum()*100//len(y_bin_one), '%')\n",
    "print('bin_all (0, 1): ', (y_bin_all == 0).sum()*100//len(y_bin_all),'%, ', (y_bin_all == 1).sum()*100//len(y_bin_all), '%')\n",
    "tmp = [0, 0, 0, 0, 0]\n",
    "for item in y_multi_maj:\n",
    "    tmp[0] += item[0];    tmp[1] += item[1];     tmp[2] += item[2];    tmp[3] += item[3];    tmp[4] += item[4]\n",
    "print('multi_maj %: ',tmp[0]*100//sum(tmp), tmp[1]*100//sum(tmp), tmp[2]*100//sum(tmp), tmp[3]*100//sum(tmp), tmp[4]*100//sum(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93755990-ff85-4dad-b263-d23451b3bf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7984"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text to sequence of words\n",
    "X_seq = []\n",
    "for item in X:\n",
    "    word_seq = tf.keras.preprocessing.text.text_to_word_sequence(\n",
    "        item,\n",
    "        filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "        lower=True,\n",
    "        split=' '\n",
    "    )\n",
    "    X_seq.append(word_seq)\n",
    "len(X_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e69aab1f-d2ed-4ddc-a871-55521ab068e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32362"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this integer encoding is purely based on position, you can do this in other ways\n",
    "#integer_mapping = {}\n",
    "\n",
    "# Flatten the list of word sequences to get a list of all words\n",
    "all_words = [word for seq in X_seq for word in seq]\n",
    "\n",
    "# Create a mapping from each unique word to a unique integer\n",
    "unique_words = set(all_words)\n",
    "integer_mapping = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "# Convert each word sequence to a sequence of integers using the mapping\n",
    "X_int_seq = [[integer_mapping[word] for word in seq] for seq in X_seq]\n",
    "\n",
    "# Example outputs\n",
    "print(len(X_int_seq))  # Original word sequences\n",
    "len(integer_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b98384f1-2d02-4be5-b70d-78d6792b7893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.862850701402806, 173, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcount = []\n",
    "for seq in X_seq:\n",
    "    wcount.append(len(seq))\n",
    "sum(wcount)/len(wcount), max(wcount), min(wcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ff3f340-be12-4974-98e9-8ad829541ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Architecture imports\n",
    "#from tensorflow.python import ops\n",
    "#from keras import ops\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9f16fa-da51-458e-a0b1-12f0740290a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(integer_mapping)  # considering all vocabulary\n",
    "maxlen = 80  # Only consider the first 60 words - max is about 100, average 33.158\n",
    "\n",
    "# Load and preprocess the data\n",
    "#(x_train, y_train), (x_val, y_val) = tf.keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "#x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "#x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "# Convert the integer sequences to numpy arrays and pad them\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_int_seq, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7240f9aa-5010-46cc-a10a-e746bfaf79e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "#num_heads = 2  # Number of attention heads\n",
    "#ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "# Load pretrained FastText embeddings\n",
    "fasttext_path = 'pretrained_embeddings/cc.de.300.vec'\n",
    "embedding_dim = 300\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(fasttext_path, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eb9bf7f-06a0-4d36-b694-871c7925d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab_size = len(unique_words)\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in integer_mapping.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2737294a-f41b-4ddf-a436-4b17b80a1656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32362, 300)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e4c74c-6ec9-4531-bac0-ca42e205da21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khantr/.local/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2024-06-29 03:31:52.193041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-29 03:31:52.193536: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Create an Embedding layer with pretrained weights\n",
    "embedding_dim = 300\n",
    "embedding_layer = layers.Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embedding_dim,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=maxlen,\n",
    "                                   trainable=False)  # Set to True to fine-tune the embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94d5786d-e976-4a2c-a24f-d699b440e57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    #def call(self, inputs, training):\n",
    "    def call(self, inputs, training=None, **kwargs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91341075-22f5-4e8e-8f35-07cf66b0f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - Architecture\n",
    "\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 30  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "#Model Architecture\n",
    "#inputs = layers.Input(shape=(maxlen,))\n",
    "# Define input layers for the text and additional features\n",
    "text_input = layers.Input(shape=(maxlen,))\n",
    "features_input = layers.Input(shape=(additional_features.shape[1],))\n",
    "x = embedding_layer(text_input)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Concatenate the processed text with additional features\n",
    "x = layers.Concatenate()([x, features_input])\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(1000, activation=\"leaky_relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(100, activation=\"leaky_relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "#x = layers.Dense(50, activation = \"relu\")(x)\n",
    "#outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# Separate output layers for each component of the target variable\n",
    "output_bin_maj = layers.Dense(1, activation=\"sigmoid\", name=\"output_bin_maj\")(x)\n",
    "output_bin_one = layers.Dense(1, activation=\"sigmoid\", name=\"output_bin_one\")(x)\n",
    "output_bin_all = layers.Dense(1, activation=\"sigmoid\", name=\"output_bin_all\")(x)\n",
    "#output_multi_maj = layers.Dense(5, activation=\"softmax\", name=\"output_multi_maj\")(x)  # Assuming the multi-majority is a multi-class problem\n",
    "output_disagree_bin = layers.Dense(1, activation=\"sigmoid\", name=\"output_disagree_bin\")(x)\n",
    "output_dist_bin_0 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_bin_0\")(x)\n",
    "output_dist_bin_1 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_bin_1\")(x)\n",
    "output_dist_multi_0 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_multi_0\")(x)\n",
    "output_dist_multi_1 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_multi_1\")(x)\n",
    "output_dist_multi_2 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_multi_2\")(x)\n",
    "output_dist_multi_3 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_multi_3\")(x)\n",
    "output_dist_multi_4 = layers.Dense(1, activation=\"sigmoid\", name=\"output_dist_multi_4\")(x)\n",
    "\n",
    "\n",
    "#model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model = tf.keras.Model(inputs=[text_input,features_input], outputs=[output_bin_maj, output_bin_one, output_bin_all, #output_multi_maj, \n",
    "                                                                    output_disagree_bin,\n",
    "                                               output_dist_bin_0, output_dist_bin_1, output_dist_multi_0, output_dist_multi_1,\n",
    "                                               output_dist_multi_2, output_dist_multi_3, output_dist_multi_4])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a3af844-04fa-4b0e-b69c-bf4fe22827d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.compile(optimizer=\"adamW\",\n",
    "              loss={\"output_bin_maj\": \"binary_focal_crossentropy\", #\"binary_crossentropy\", \n",
    "                    \"output_bin_one\": \"binary_focal_crossentropy\",\n",
    "                    \"output_bin_all\": \"binary_focal_crossentropy\",\n",
    "                    #\"output_multi_maj\": \"categorical_focal_crossentropy\", #\"categorical_crossentropy\",\n",
    "                    \"output_disagree_bin\": \"binary_focal_crossentropy\",\n",
    "                    \"output_dist_bin_0\": \"mse\",\n",
    "                    \"output_dist_bin_1\": \"mse\",\n",
    "                    \"output_dist_multi_0\": \"mse\",\n",
    "                    \"output_dist_multi_1\": \"mse\",\n",
    "                    \"output_dist_multi_2\": \"mse\",\n",
    "                    \"output_dist_multi_3\": \"mse\",\n",
    "                    \"output_dist_multi_4\": \"mse\"},\n",
    "              metrics={\"output_bin_maj\": \"binary_accuracy\", \n",
    "                       \"output_bin_one\": \"binary_accuracy\",\n",
    "                       \"output_bin_all\": \"binary_accuracy\",\n",
    "                       #\"output_multi_maj\": \"categorical_accuracy\",\n",
    "                       \"output_disagree_bin\": \"binary_accuracy\",\n",
    "                       \"output_dist_bin_0\": \"mape\",\n",
    "                       \"output_dist_bin_1\": \"mape\",\n",
    "                       \"output_dist_multi_0\": \"mape\",\n",
    "                       \"output_dist_multi_1\": \"mape\",\n",
    "                       \"output_dist_multi_2\": \"mape\",\n",
    "                       \"output_dist_multi_3\": \"mape\",\n",
    "                       \"output_dist_multi_4\": \"mape\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf714fd3-9145-4e76-9132-758a75d3451c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5998"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded = X_padded[:numTrainSamples]\n",
    "X_test_padded = X_padded[numTrainSamples:]\n",
    "numTrainSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62aa6939-3513-442e-bbef-12f54e8070ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 71ms/step - loss: 8.9653 - output_bin_all_binary_accuracy: 0.7099 - output_bin_maj_binary_accuracy: 0.5567 - output_bin_one_binary_accuracy: 0.4937 - output_disagree_bin_binary_accuracy: 0.5358 - output_dist_bin_0_mape: 134405312.0000 - output_dist_bin_1_mape: 138979184.0000 - output_dist_multi_0_mape: 91743088.0000 - output_dist_multi_1_mape: 103356208.0000 - output_dist_multi_2_mape: 96827640.0000 - output_dist_multi_3_mape: 94575400.0000 - output_dist_multi_4_mape: 131812984.0000 - val_loss: 3.1377 - val_output_bin_all_binary_accuracy: 0.8408 - val_output_bin_maj_binary_accuracy: 0.5692 - val_output_bin_one_binary_accuracy: 0.3883 - val_output_disagree_bin_binary_accuracy: 0.5167 - val_output_dist_bin_0_mape: 146157888.0000 - val_output_dist_bin_1_mape: 45868972.0000 - val_output_dist_multi_0_mape: 141338944.0000 - val_output_dist_multi_1_mape: 13644878.0000 - val_output_dist_multi_2_mape: 23181416.0000 - val_output_dist_multi_3_mape: 27548196.0000 - val_output_dist_multi_4_mape: 25736608.0000\n",
      "Epoch 2/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - loss: 3.1675 - output_bin_all_binary_accuracy: 0.7235 - output_bin_maj_binary_accuracy: 0.5630 - output_bin_one_binary_accuracy: 0.5079 - output_disagree_bin_binary_accuracy: 0.5611 - output_dist_bin_0_mape: 119964408.0000 - output_dist_bin_1_mape: 123959384.0000 - output_dist_multi_0_mape: 125055928.0000 - output_dist_multi_1_mape: 49090032.0000 - output_dist_multi_2_mape: 55876080.0000 - output_dist_multi_3_mape: 58994728.0000 - output_dist_multi_4_mape: 44417712.0000 - val_loss: 2.1338 - val_output_bin_all_binary_accuracy: 0.8408 - val_output_bin_maj_binary_accuracy: 0.5692 - val_output_bin_one_binary_accuracy: 0.3892 - val_output_disagree_bin_binary_accuracy: 0.5483 - val_output_dist_bin_0_mape: 130288424.0000 - val_output_dist_bin_1_mape: 108539592.0000 - val_output_dist_multi_0_mape: 133764304.0000 - val_output_dist_multi_1_mape: 28053976.0000 - val_output_dist_multi_2_mape: 51183672.0000 - val_output_dist_multi_3_mape: 49917296.0000 - val_output_dist_multi_4_mape: 33260186.0000\n",
      "Epoch 3/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - loss: 2.2257 - output_bin_all_binary_accuracy: 0.7538 - output_bin_maj_binary_accuracy: 0.5970 - output_bin_one_binary_accuracy: 0.4967 - output_disagree_bin_binary_accuracy: 0.5724 - output_dist_bin_0_mape: 117929424.0000 - output_dist_bin_1_mape: 131294400.0000 - output_dist_multi_0_mape: 114154856.0000 - output_dist_multi_1_mape: 48770656.0000 - output_dist_multi_2_mape: 64334108.0000 - output_dist_multi_3_mape: 57346204.0000 - output_dist_multi_4_mape: 44681340.0000 - val_loss: 1.3789 - val_output_bin_all_binary_accuracy: 0.8408 - val_output_bin_maj_binary_accuracy: 0.5900 - val_output_bin_one_binary_accuracy: 0.5625 - val_output_disagree_bin_binary_accuracy: 0.5558 - val_output_dist_bin_0_mape: 123388656.0000 - val_output_dist_bin_1_mape: 98719664.0000 - val_output_dist_multi_0_mape: 129186408.0000 - val_output_dist_multi_1_mape: 39059752.0000 - val_output_dist_multi_2_mape: 60019220.0000 - val_output_dist_multi_3_mape: 75215480.0000 - val_output_dist_multi_4_mape: 26154526.0000\n",
      "Epoch 4/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 69ms/step - loss: 1.7030 - output_bin_all_binary_accuracy: 0.7700 - output_bin_maj_binary_accuracy: 0.6000 - output_bin_one_binary_accuracy: 0.5082 - output_disagree_bin_binary_accuracy: 0.5943 - output_dist_bin_0_mape: 116363536.0000 - output_dist_bin_1_mape: 146143952.0000 - output_dist_multi_0_mape: 113477096.0000 - output_dist_multi_1_mape: 47501836.0000 - output_dist_multi_2_mape: 72784088.0000 - output_dist_multi_3_mape: 63080436.0000 - output_dist_multi_4_mape: 40017472.0000 - val_loss: 1.2768 - val_output_bin_all_binary_accuracy: 0.8408 - val_output_bin_maj_binary_accuracy: 0.5700 - val_output_bin_one_binary_accuracy: 0.5308 - val_output_disagree_bin_binary_accuracy: 0.5425 - val_output_dist_bin_0_mape: 107184200.0000 - val_output_dist_bin_1_mape: 131028504.0000 - val_output_dist_multi_0_mape: 110818992.0000 - val_output_dist_multi_1_mape: 49248952.0000 - val_output_dist_multi_2_mape: 77560064.0000 - val_output_dist_multi_3_mape: 84349832.0000 - val_output_dist_multi_4_mape: 42288144.0000\n",
      "Epoch 5/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 67ms/step - loss: 1.4509 - output_bin_all_binary_accuracy: 0.7937 - output_bin_maj_binary_accuracy: 0.5976 - output_bin_one_binary_accuracy: 0.5288 - output_disagree_bin_binary_accuracy: 0.5877 - output_dist_bin_0_mape: 108961056.0000 - output_dist_bin_1_mape: 149311408.0000 - output_dist_multi_0_mape: 110009472.0000 - output_dist_multi_1_mape: 42328512.0000 - output_dist_multi_2_mape: 71073656.0000 - output_dist_multi_3_mape: 61672980.0000 - output_dist_multi_4_mape: 34770772.0000 - val_loss: 1.2466 - val_output_bin_all_binary_accuracy: 0.8408 - val_output_bin_maj_binary_accuracy: 0.5692 - val_output_bin_one_binary_accuracy: 0.5975 - val_output_disagree_bin_binary_accuracy: 0.5483 - val_output_dist_bin_0_mape: 104895504.0000 - val_output_dist_bin_1_mape: 127832336.0000 - val_output_dist_multi_0_mape: 104520400.0000 - val_output_dist_multi_1_mape: 47872936.0000 - val_output_dist_multi_2_mape: 67365256.0000 - val_output_dist_multi_3_mape: 78184888.0000 - val_output_dist_multi_4_mape: 41774568.0000\n"
     ]
    }
   ],
   "source": [
    "### Train the model\n",
    "history = model.fit([X_train_padded,additional_features], {\"output_bin_maj\": y_bin_maj,\n",
    "                                     \"output_bin_one\": y_bin_one,\n",
    "                                     \"output_bin_all\": y_bin_all,\n",
    "                                     #\"output_multi_maj\": y_multi_maj,\n",
    "                                     \"output_disagree_bin\": y_disagree_bin,\n",
    "                                     \"output_dist_bin_0\": y_dist_bin_0,\n",
    "                                     \"output_dist_bin_1\": y_dist_bin_1,\n",
    "                                     \"output_dist_multi_0\": y_dist_multi_0,\n",
    "                                     \"output_dist_multi_1\": y_dist_multi_1,\n",
    "                                     \"output_dist_multi_2\": y_dist_multi_2,\n",
    "                                     \"output_dist_multi_3\": y_dist_multi_3,\n",
    "                                     \"output_dist_multi_4\": y_dist_multi_4},\n",
    "                                     #\"output_multi_maj\": np.array(y_multi_maj)},\n",
    "                    #sample_weight=sample_weights_multi_maj,\n",
    "                    batch_size=40, epochs=5, validation_split=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33214bbb-3b40-4f7f-ac65-dc734ce128d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ebbfb25c-740d-48c3-8110-30c412362eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 66ms/step - loss: 4.1450 - output_disagree_bin_binary_accuracy: 0.5355 - output_multi_maj_categorical_accuracy: 0.5279 - val_loss: 0.5644 - val_output_disagree_bin_binary_accuracy: 0.5450 - val_output_multi_maj_categorical_accuracy: 0.6508\n",
      "Epoch 2/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - loss: 1.1726 - output_disagree_bin_binary_accuracy: 0.5392 - output_multi_maj_categorical_accuracy: 0.5673 - val_loss: 0.7783 - val_output_disagree_bin_binary_accuracy: 0.5483 - val_output_multi_maj_categorical_accuracy: 0.6508\n",
      "Epoch 3/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - loss: 0.9077 - output_disagree_bin_binary_accuracy: 0.5714 - output_multi_maj_categorical_accuracy: 0.5941 - val_loss: 0.3807 - val_output_disagree_bin_binary_accuracy: 0.5483 - val_output_multi_maj_categorical_accuracy: 0.6508\n",
      "Epoch 4/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - loss: 0.6527 - output_disagree_bin_binary_accuracy: 0.5519 - output_multi_maj_categorical_accuracy: 0.5988 - val_loss: 0.5625 - val_output_disagree_bin_binary_accuracy: 0.5483 - val_output_multi_maj_categorical_accuracy: 0.6508\n",
      "Epoch 5/5\n",
      "\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 64ms/step - loss: 0.4977 - output_disagree_bin_binary_accuracy: 0.5781 - output_multi_maj_categorical_accuracy: 0.6440 - val_loss: 0.3767 - val_output_disagree_bin_binary_accuracy: 0.5092 - val_output_multi_maj_categorical_accuracy: 0.6508\n"
     ]
    }
   ],
   "source": [
    "text_input = layers.Input(shape=(maxlen,))\n",
    "features_input = layers.Input(shape=(additional_features.shape[1],))\n",
    "x = embedding_layer(text_input)\n",
    "transformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# Concatenate the processed text with additional features\n",
    "x = layers.Concatenate()([x, features_input])\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(1000, activation=\"leaky_relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "x = layers.Dense(100, activation=\"leaky_relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "#x = layers.Dense(50, activation = \"relu\")(x)\n",
    "#outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# Separate output layers for each component of the target variable\n",
    "output_multi_maj = layers.Dense(5, activation=\"softmax\", name=\"output_multi_maj\")(x)  # Assuming the multi-majority is a multi-class problem\n",
    "output_disagree_bin = layers.Dense(1, activation=\"sigmoid\", name=\"output_disagree_bin\")(x)\n",
    "\n",
    "#model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model2 = tf.keras.Model(inputs=[text_input,features_input], outputs=[output_multi_maj, output_disagree_bin])\n",
    "#model.summary()\n",
    "#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model2.compile(optimizer=\"adamW\",\n",
    "              loss={\"output_multi_maj\": \"categorical_focal_crossentropy\", #\"categorical_crossentropy\",\n",
    "                   \"output_disagree_bin\": \"binary_focal_crossentropy\"},\n",
    "              metrics={\"output_multi_maj\": \"categorical_accuracy\",\n",
    "                      \"output_disagree_bin\" : \"binary_accuracy\"})\n",
    "### Train the model\n",
    "history2 = model2.fit([X_train_padded,additional_features], {\"output_multi_maj\": y_multi_maj, \"output_disagree_bin\" : y_disagree_bin},\n",
    "                                     #\"output_multi_maj\": np.array(y_multi_maj)},\n",
    "                    #sample_weight=sample_weights_multi_maj,\n",
    "                    batch_size=40, epochs=5, validation_split=0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "714f3f53-2ad5-480d-83ad-c516990ea0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_models/development_model4_final.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec923b1-5375-48ec-8223-f764f6ca885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = keras.saving.load_model(\"saved_models/trial_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24830fc5-3fe6-421b-9cec-4326935b93ff",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d1afab-84e4-4a65-a357-29166fa37d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_test_padded,additional_features_test][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a724324-c6e1-494a-a3fa-eabea08d717d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42225596], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred2[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b969a071-b9e4-4070-9511-89eb02f8c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict([X_test_padded,additional_features_test], batch_size=None, verbose='auto', steps=None, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3ae80e37-c65f-480c-9369-198e04291ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n"
     ]
    }
   ],
   "source": [
    "ypred2 = model2.predict([X_test_padded,additional_features_test], batch_size=None, verbose='auto', steps=None, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28cbd964-7343-4756-9bcd-ef2c0e847fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1986"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3f294-1506-4ef2-bcfa-f95d1483123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output to DF\n",
    "test_id = [json_test_data[i]['id'] for i in range(len(json_test_data))]\n",
    "#ypred_bin_maj = [ypred[0][i][0] for i in range(len(ypred[0]))]\n",
    "ypred_bin_maj = [1 if ypred[0][i][0] >= 0.5 else 0 for i in range(len(ypred[0]))]\n",
    "ypred_bin_one = [1 if ypred[1][i][0] >= 0.5 else 0 for i in range(len(ypred[1]))]\n",
    "ypred_bin_all = [1 if ypred[2][i][0] >= 0.5 else 0 for i in range(len(ypred[2]))]\n",
    "categories = [\"0-Kein\", \"1-Gering\", \"2-Vorhanden\", \"3-Stark\", \"4-Extrem\"]\n",
    "tmp = [categories[ll.index(max(ll))] for ll in [list(ypred[0][i]) for i in range(len(ypred[3]))]]\n",
    "\n",
    "# Predictions for multi_maj\n",
    "ypred_multi_maj_raw = tmp  # This is the raw output from the model for multi_maj\n",
    "print(ypred_multi_maj_raw[0])\n",
    "\n",
    "ypred_disagree_bin = [1 if ypred[4][i][0] >= 0.5 else 0 for i in range(len(ypred[4]))]\n",
    "\n",
    "ypred_dist_bin_0 = [ypred[5][i][0] for i in range(len(ypred[5]))]\n",
    "ypred_dist_bin_1 = [round(ypred[6][i][0],4) for i in range(len(ypred[6]))]\n",
    "for i in range(len(ypred_dist_bin_0)):\n",
    "    tt = ypred_dist_bin_0[i] + ypred_dist_bin_1[i]\n",
    "    ypred_dist_bin_0[i] = ypred_dist_bin_0[i] / tt\n",
    "    ypred_dist_bin_1[i] = ypred_dist_bin_1[i] / tt\n",
    "\n",
    "ypred_dist_multi_0 = [ypred[7][i][0] for i in range(len(ypred[7]))]\n",
    "ypred_dist_multi_1 = [ypred[8][i][0] for i in range(len(ypred[8]))]\n",
    "ypred_dist_multi_2 = [ypred[9][i][0] for i in range(len(ypred[9]))]\n",
    "ypred_dist_multi_3 = [ypred[10][i][0] for i in range(len(ypred[10]))]\n",
    "ypred_dist_multi_4 = [ypred[11][i][0] for i in range(len(ypred[11]))]\n",
    "for i in range(len(ypred_dist_multi_0)):\n",
    "    tt = ypred_dist_multi_0[i] + ypred_dist_multi_1[i] + ypred_dist_multi_2[i] + ypred_dist_multi_3[i] + ypred_dist_multi_4[i]  \n",
    "    ypred_dist_multi_0[i] = ypred_dist_multi_0[i] / tt\n",
    "    ypred_dist_multi_1[i] = ypred_dist_multi_1[i] / tt\n",
    "    ypred_dist_multi_2[i] = ypred_dist_multi_2[i] / tt\n",
    "    ypred_dist_multi_3[i] = ypred_dist_multi_3[i] / tt\n",
    "    ypred_dist_multi_4[i] = ypred_dist_multi_4[i] / tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "33a7ab02-eb9f-4995-9e46-f4d262cbfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ypred_multi_maj_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "179b091c-b6cd-47f8-a66b-fa8f59be2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93fc1292-c2a6-4bcd-bb26-44db3a7b9294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_subtask1 = pd.DataFrame({\n",
    "              'id': test_id,\n",
    "             'bin_maj': ypred_bin_maj,\n",
    "             'bin_one': ypred_bin_one,\n",
    "              'bin_all': ypred_bin_all,\n",
    "              'multi_maj': ypred_multi_maj_raw,\n",
    "              'disagree_bin': ypred_disagree_bin,\n",
    "             })\n",
    "df_output_subtask1.head(2)\n",
    "df_output_subtask2 = pd.DataFrame({\n",
    "                    'id': test_id,\n",
    "                  'dist_bin_0': ypred_dist_bin_0,\n",
    "              'dist_bin_1': ypred_dist_bin_1,\n",
    "              'dist_multi_0': ypred_dist_multi_0,\n",
    "              'dist_multi_1': ypred_dist_multi_1,\n",
    "              'dist_multi_2': ypred_dist_multi_2,\n",
    "              'dist_multi_3': ypred_dist_multi_3,\n",
    "              'dist_multi_4': ypred_dist_multi_4\n",
    "\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a4d595ba-dc29-4d60-b0d4-3404cfa9bddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_output_subtask2['dist_multi_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "94ba28fe-53ee-4187-9dc0-f09cea3bef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed subtask 1 output file\n",
    "subtask1_output = df_output_subtask1.to_csv('data/challenge_data/subtask1_output_model2.tsv', sep=\"\\t\", index = False) \n",
    "subtask2_output = df_output_subtask2.to_csv('data/challenge_data/subtask2_output_model2.tsv', sep=\"\\t\", index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9522a9da-70a7-4c2c-b3f7-399a6e9400d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0132e99-9d50-4f25-81ed-241c49707679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da249411-b39f-41c7-8591-e6651a4127bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
